<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Trong Thang Pham</title>

  <meta name="author" content="Trong-Thang Pham">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Trong Thang Pham</name>
                  </p>
                  <p>
                    Hi! I'm Thang. Currently, I'm a PhD candidate at the University of Arkansas, Fayetteville
                    under the supervision of Dr. <a  href="https://scholar.google.com/citations?user=8ck0k_UAAAAJ&hl=en">Ngan Le</a>. During my PhD, I research applying human eye gaze analysis to transform deep learning models based on human behavior. So far, I have contributed 7 papers to the medical field during my PhD.
                      
                    <!-- I got my bachelor's degree in Computer Science at University of Science, VNU-HCM, Vietnam.  -->
                    <!-- and studying a master's degree in Artificial Intelligence at John von Neumann Institute, VNU-HCM.  -->
                    I used to work as a core R&D researcher at AIOZ Singapore, developing digital twin technologies including talking face generation and 3D human models for metaverse applications. Our team at AIOZ published two papers at CVPRW and CVPR.
                    In summary, I have a strong publications in many top conferences, i.e. CVPR, ICCV, ACM MM, WACV, and ICRA, and journals, i.e. Image and Vision Computing (IVC) and Artificial Intelligence in Medicine (AIM) at Elsevier.
                  </p>
                  <!-- <p> -->
                    <!-- Email: <br> -->
                    <!-- - Personal: <a href="mailto:phamtrongthang123@gmail.com">phamtrongthang123@gmail.com</a><br> -->
                    <!-- - John von Neumann Institute: <a href="mailto:thang.pham2021@ict.jvn.edu.vn">thang.pham2021@ict.jvn.edu.vn</a><br> -->
                    <!-- - University of Arkansas: <a href="mailto:tp030@uark.edu">tp030@uark.edu</a><br> -->
                  <!-- </p> -->
                  
                  I genuinely love receiving emails and hearing from fellow researchers, students, engineers, and anyone interested in computer vision, medical AI, or related fields. Whether you want to discuss my research, share what you're working on, explore potential collaborations, or just chat about interesting ideas in our field, please don't hesitate to reach out at 
                  <a href="mailto:phamtrongthang123@gmail.com" style="color: #007bff; text-decoration: none;">phamtrongthang123@gmail.com</a> or 
                  <a href="mailto:tp030@uark.edu" style="color: #007bff; text-decoration: none;">tp030@uark.edu</a>
                  <p style="text-align:center">

                    <a
                      href="https://github.com/phamtrongthang123/phamtrongthang123.github.io/blob/main/files/cv_resume%20(3).pdf">CV</a>
                    &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=SwR0T0UAAAAJ">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://twitter.com/trongthangpham">Twitter</a> &nbsp/&nbsp
                    <a href="https://github.com/phamtrongthang123">Github</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/phamtrongthanghcmus/">Linkedin</a> &nbsp/&nbsp
                    <a href="https://trongthangpham.substack.com/">Substack</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/avatar.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/avatar.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- <section style="margin: 2rem 0; padding: 1.5rem; background-color: #f8f9fa; border-radius: 8px; border-left: 4px solid #007bff;">
              <h2 style="color: #333; margin-bottom: 1rem; font-size: 1.4rem;">Standing Invitation: if you want to talk research, I want to talk to you.</h2>
              <p style="line-height: 1.6; color: #555; margin: 0; font-size: 0.9rem;">
                  I genuinely love receiving emails and hearing from fellow researchers, students, engineers, and anyone interested in computer vision, medical AI, or related fields. Whether you want to discuss my research, share what you're working on, explore potential collaborations, or just chat about interesting ideas in our field, please don't hesitate to reach out at 
                  <a href="mailto:phamtrongthang123@gmail.com" style="color: #007bff; text-decoration: none;">phamtrongthang123@gmail.com</a> or 
                  <a href="mailto:tp030@uark.edu" style="color: #007bff; text-decoration: none;">tp030@uark.edu</a>. 
                  While I can't promise immediate responses due to research commitments, I read everything and do my best to reply thoughtfully.
              </p>
          </section> -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research Interests</heading>
                  <p>
                    <strong>I am interested in human behavior in professional settings. How do professionals behave during high-stake decision tasks? What cognitive goals drive their behavioral patterns?</strong> My work combines computer vision, interpretable AI, and deep learning to develop new models to capture and mimic human behavior in workplace environments. My current PhD work focuses on the <strong>medical/cardiology domain</strong> as the professional setting.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <ul style="line-height:1.8;">
                    <li> <strong>[July 2025]</strong> üéâ Our paper "CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling" has been accepted to <span style="color: red; font-weight: bold;">ICCV 2025 as Highlight!</span></li>
                    <li> <strong>[July 2025]</strong> üéâ Our paper "Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis" has been accepted to <span style="color: red; font-weight: bold;">ACM MM 2025 as Oral!</span></li>
                    <li> <strong>[July 2025]</strong> üìù Our paper "TolerantECG: A Foundation Model for Imperfect Electrocardiogram" has been accepted to ACM MM 2025!</li>
                    <li> <strong>[January 2025]</strong> üìù Our paper "A2VIS: Amodal-Aware Approach to Video Instance Segmentation" has been accepted to Image and Vision Computing (IVC), Elsevier!</li>
                    <li> <strong>[November 2024]</strong> üìù Our paper "ItpCtrl-AI: End-to-end interpretable and controllable artificial intelligence by modeling radiologists' intentions" has been accepted to Artificial Intelligence in Medicine (AIM), Elsevier!</li>
                    <li> <strong>[October 2024]</strong> üéâ Our paper "GazeSearch: Radiology Findings Search Benchmark" has been accepted to <span style="color: red; font-weight: bold;">WACV 2025 as Oral!</span></li>
                    <li> <strong>[September 2024]</strong> üìù Our paper "FG-CXR: A Radiologist-Aligned Gaze Dataset for Enhancing Interpretability in Chest X-Ray Report Generation" has been accepted to ACCV 2024!</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/iccv2025.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling</papertitle>
                  <br>
                  <strong>Trong Thang Pham</strong>, Akash Awasthi, Saba Khan, Esteban Duran Marti, Tien-Phat Nguyen, Khoa Vo, Minh Tran, Ngoc Son Nguyen, Cuong Tran Van, Yuki Ikebe, Anh Totti Nguyen, Anh Nguyen, Zhigang Deng, Carol C. Wu, Hien Van Nguyen, Ngan Le
                  <br>
                  <em>IEEE/CVF International Conference on Computer Vision (ICCV) <span
                      style="color: red; font-weight: bold;">(Highlight 2025)</span></em>
                  <br>
                  <a href="https://arxiv.org/html/2507.12591v1">[Paper]</a> <a href="https://github.com/UARK-AICV/CTScanGaze">[Code]</a>
                  <p></p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/acmmm2025_eye.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis</papertitle>
                  <br>
                  <strong>Trong Thang Pham</strong>, Anh Nguyen, Zhigang Deng, Carol C. Wu, Hien Van Nguyen, Ngan Le
                  <br>
                  <em>Proceedings of the ACM International Conference on Multimedia (ACMMM) <span
                      style="color: red; font-weight: bold;">(Oral 2025)</span></em>
                  <br>
                  <a href="https://arxiv.org/abs/2507.12461">[Paper]</a> <a href="https://github.com/UARK-AICV/RadGazeIntent">[Code]</a>
                  <p></p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/ivc2025.jpg' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>A2VIS: Amodal-Aware Approach to Video Instance Segmentation</papertitle>
                  <br>
                  <strong>Trong Thang Pham*</strong>, Minh Tran*, Winston Bounsavy, Tri Nguyen, Ngan Le
                  <br>
                  <em>Image and Vision Computing, Elsevier, 2025 (Q1, IF 4.2)</em>
                  <br>
                  <em>* same contribution</em>
                  <br>
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885625001313">[Paper]</a>
                  <p></p>
                </td>
              </tr>
              <tr> 
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/itpctrl-ai.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>ItpCtrl-AI: End-to-end interpretable and controllable artificial intelligence by modeling radiologists' intentions</papertitle>
                  <br>
                  <strong>Trong-Thang Pham</strong>, Jacob Brecheisen, Carol C. Wu, Hien Nguyen, Zhigang Deng, Donald Adjeroh, Gianfranco Doretto, Arabinda Choudhary, Ngan Le
                  <br>
                  <em>Artificial Intelligence in Medicine, Volume 160, February 2025 (Q1, IF 6.1)</em>
                  <br>
                  <a href="https://www.sciencedirect.com/science/article/pii/S0933365724002963">[Paper]</a>
                  <p></p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/wacv2025.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>GazeSearch: Radiology Findings Search Benchmark</papertitle>
                  <br>
                  <strong>Trong Thang Pham</strong>,Tien-Phat Nguyen, Yuki Ikebe, Akash Awasthi, Zhigang Deng, Carol C.
                  Wu, Hien Nguyen, Ngan Le
                  <br>
                  <em>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) <span
                      style="color: red; font-weight: bold;">(Oral 2025)</span></em>
                  <br>
                  <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Pham_GazeSearch_Radiology_Findings_Search_Benchmark_WACV_2025_paper.pdf">[Paper]</a> <a href="https://github.com/UARK-AICV/GazeSearch">[Code]</a> 
                  <p></p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/accv2024.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>FG-CXR: A Radiologist-Aligned Gaze Dataset for Enhancing Interpretability in Chest X-Ray
                    Report Generation</papertitle>
                  <br>
                  <strong>Trong Thang Pham</strong>, Ngoc-Vuong Ho, Nhat-Tan Bui, Thinh Phan, Patel Brijesh, Donald
                  Adjeroh, Gianfranco Doretto, Anh Nguyen, Carol C. Wu, Hien Nguyen, Ngan Le
                  <br>
                  <em>Asian Conference on Computer Vision (ACCV) (2024)</em>
                  <br>
                  <a href="https://arxiv.org/pdf/2411.15413">[Paper]</a> <a href="https://github.com/UARK-AICV/FG-CXR">[Code]</a>
                  <p></p> 
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/wacv2024.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Decoding Radiologists Intense Focus for Accurate CXR Diagnoses: A Controllable and
                    Interpretable AI System.</papertitle>
                  <br>
                  <strong>Trong Thang Pham</strong>, Jacob Brecheisen, Anh Nguyen, Hien Nguyen, and Ngan Le
                  <br>
                  <em>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024</em>
                  <br>
                  <a
                    href="https://openaccess.thecvf.com/content/WACV2024/papers/Pham_I-AI_A_Controllable__Interpretable_AI_System_for_Decoding_Radiologists_WACV_2024_paper.pdf">[Paper]</a> <a href="https://github.com/UARK-AICV/IAI">[Code]</a>
                  <p></p>
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Other Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/acmmm2025_ecg.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>TolerantECG: A Foundation Model for Imperfect Electrocardiogram</papertitle>
                  <br>
                  Huynh Nguyen Dang, <strong>Trong-Thang Pham</strong>, Ngan Le, Van Nguyen
                  <br>
                  <em>Proceedings of the ACM International Conference on Multimedia (ACMMM) 2025</em>
                  <br>
                  <a href="https://arxiv.org/abs/2507.09887">[Paper]</a>
                  <p></p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/icra2024.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation.
                  </papertitle>
                  <br>
                  Yamazaki, Kashu, Taisei Hanyu, Khoa Vo, <strong>Trong Thang Pham</strong>, Minh Tran, Gianfranco
                  Doretto, Anh Nguyen, and Ngan Le
                  <br>
                  <em>International Conference on Robotics and Automation (ICRA) 2024</em>
                  <br>
                  <a href="https://arxiv.org/pdf/2310.03923.pdf">[Paper]</a> <a href="https://github.com/UARK-AICV/OpenFusion">[Code]</a>
                  <p></p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/cvprw2023.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>DNA: Deformable Neural Articulations Network for Template-Free Dynamic 3D Human
                    Reconstruction From Monocular RGB-D Video.</papertitle>
                  <br>
                  Vo, Khoa, <strong>Trong Thang Pham</strong>, Kashu Yamazaki, Minh Tran, and Ngan Le
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) 2023</em>
                  <br>
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2023W/Precognition/papers/Vo_DNA_Deformable_Neural_Articulations_Network_for_Template-Free_Dynamic_3D_Human_CVPRW_2023_paper.pdf">[Paper]</a>
                  <p></p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/cvpr2023.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Music-Driven Group Choreography</papertitle>
                  <br>
                  Le, Nhat, <strong>Trong Thang Pham</strong>, Tuong Do, Erman Tjiputra, Quang D. Tran, and Anh Nguyen
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023</em>
                  <br>
                  <a href="https://aioz-ai.github.io/AIOZ-GDANCE/">[Paper]</a> <a href="https://github.com/aioz-ai/AIOZ-GDANCE">[Dataset]</a>
                  <p></p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/wacv2023.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>EmbryosFormer: Deformable Transformer and Collaborative Encoding-Decoding for Embryos
                    Stage Development Classification.</papertitle>
                  <br>
                  Nguyen, Tien-Phat, <strong>Trong Thang Pham</strong>, Tri Nguyen, Hieu Le, Dung Nguyen, Hau Lam, Phong
                  Nguyen, Jennifer Fowler, Minh-Triet Tran, and Ngan Le
                  <br>
                  <em>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2023</em>
                  <br>
                  <a
                    href="https://openaccess.thecvf.com/content/WACV2023/papers/Nguyen_EmbryosFormer_Deformable_Transformer_and_Collaborative_Encoding-Decoding_for_Embryos_Stage_Development_WACV_2023_paper.pdf">[Paper]</a>
                  <p></p>
                </td>
              </tr>


              <!-- <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/aicity2020.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>HCMUS at Pixel Privacy 2020: Quality Camouflage with Back Propagation and Image
                    Enhancement.</papertitle>
                  <br>
                  Minh-Triet Tran, Tam V Nguyen, Trung-Hieu Hoang, Trung-Nghia Le, Khac-Tuan Nguyen, Dat-Thanh Dinh,
                  Thanh-An Nguyen, Hai-Dang Nguyen, Xuan-Nhat Hoang, Trong-Tung Nguyen, Viet-Khoa Vo-Ho, Trong-Le Do,
                  Lam Nguyen, Minh-Quan Le, Hoang-Phuc Nguyen-Dinh, <strong> Trong-Thang Pham</strong>, Xuan-Vy Nguyen,
                  E-Ro Nguyen, Quoc-Cuong Tran, Hung Tran, Hieu Dao, Mai-Khiem Tran, Quang-Thuc Nguyen, Tien-Phat
                  Nguyen, Gia-Han Diep, Minh N Do
                  <br>
                  <em>CVPRW </em>, 2020
                  <br>
                  <a
                    href="http://openaccess.thecvf.com/content_CVPRW_2020/papers/w35/Tran_iTASK_-_Intelligent_Traffic_Analysis_Software_Kit_CVPRW_2020_paper.pdf">[Paper]</a>
                  <p></p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/pixelprivacy2020.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>HCMUS at Pixel Privacy 2020: Quality Camouflage with Back Propagation and Image
                    Enhancement.</papertitle>
                  <br>
                  Minh-Khoi Pham, Hai-Tuan Ho-Nguyen, <strong> Trong-Thang Pham</strong>, Hung Vinh Tran, Hai-Dang
                  Nguyen, Minh-Triet Tran
                  <br>
                  <em>Pixel Privacy track at Multimedia Eval Workshop </em>, 2020
                  <br>
                  <a href="http://ceur-ws.org/Vol-2882/paper41.pdf">[Paper]</a>
                  <p></p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/papers/pixelprivacy2019.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>HCMUS at Pixel Privacy 2019: Scene Category Protection with Back Propagation and Image
                    Enhancement</papertitle>
                  <br>
                  Hung Vinh Tran*, <strong> Trong-Thang Pham* </strong>, Hai-Tuan Ho-Nguyen*, Hoai-Lam Nguyen-Hy*,
                  Xuan-Vy Nguyen*, Thang-Long Nguyen-Ho*, Minh-Triet Tran
                  <br>
                  <em>Pixel Privacy track at Multimedia Eval Workshop </em>, 2019
                  <br>
                  <a href="http://ceur-ws.org/Vol-2670/MediaEval_19_paper_16.pdf">[Paper]</a>
                  <p></p>
                </td>
              </tr> -->

            </tbody>
          </table>

          <!-- <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Software</heading>
                  <p>
                    - <a href="https://clean-unused-bib-entry.fly.dev/">Bibliography Cleaner</a>: Remove unused citations from .bib files. I am using free service so it may be down sometimes.<br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table> -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Professional Services</heading>
                  <p>
                    - Reviewer at The Annual Conference on Neural Information Processing Systems (NeurIPS) 2025 <br>
                    - Reviewer at The ACM International Conference on Multimedia (MM) 2025 <br>
                    - Reviewer at IEEE Transaction of Image Processing <br>
                    - Reviewer at CVPR 2024 & 2025 <br>
                    - Reviewer at ECCV 2024 <br>
                    - Reviewer at AAAI 2025 <br>
                    - Reviewer at WACV 2025 <br>
                    - Reviewer at ACCV 2024 <br>
                    - Reviewer Cv4animals Workshop at CVPR 2024 <br>
                  </p>
                  <heading>Teaching Assistant</heading>
                  <p>
                    - CSCE 5613: Introduction to Artificial Intelligence, University of Arkansas <br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Website template from <a href="https://jonbarron.info/"> Jon Barron.</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
  
  <div class="visitor-map">
  <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=NVTNKxXuqSA-6Uwn1cj-96W3cbIcG4K81G9FCuIbtE8&cl=ffffff&w=a"></script>
  </div>

</body>

</html>